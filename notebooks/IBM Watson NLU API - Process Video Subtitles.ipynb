{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import string\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import six\n",
    "import json\n",
    "import requests\n",
    "\n",
    "named_entities = [\"storageLink\",\"entity\", \"type\", \"count\", \"entity_start_offset\", \"entity_end_offset\", \"relevance\", \"subtypes\", \"DBpedia_URL\", \"sentiment_label\", \"sentiment_score\", \"emotion_sadness\", \"emotion_fear\", \"emotion_joy\", \"emotion_disgust\", \"emotion_anger\"]\n",
    "\n",
    "url = 'https://gateway-lon.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2018-11-16'\n",
    "key = [PLACEHOLDER_API_KEY]\n",
    "\n",
    "videos = [PLACEHOLDER_LIST_OF_VIDEO_IDS]\n",
    "\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "parameters = {}\n",
    "parameters['features'] = {}\n",
    "parameters['features']['entities'] = {}\n",
    "parameters['features']['entities']['mentions'] = True\n",
    "parameters['features']['entities']['sentiment'] = True\n",
    "parameters['features']['entities']['emotion'] = True\n",
    "parameters['language'] = 'en'\n",
    "\n",
    "for video in videos:\n",
    "    \n",
    "    with open(\"..data/video_subtitles_entities/\" + video + \".csv\", \"w\" ) as outSentences:\n",
    "        writer = csv.writer( outSentences )\n",
    "        writer.writerow(named_entities)\n",
    "\n",
    "    video_subtitle_file = [PLACEHOLDER_VIDEO_SUBTITLE_CSV]\n",
    "\n",
    "    df = video_subtitle_file.groupby(by=['storageLink','transcript','transcript_part'], as_index=False).first()\n",
    "    df = df[[\"storageLink\", \"transcript\", \"transcript_part\", \"start_time\", \"end_time\"]]\n",
    "    df = df.sort_values([\"transcript_part\"])\n",
    "\n",
    "    text = \" \".join(list(df[\"transcript\"]))\n",
    "\n",
    "    parameters['text'] = text\n",
    "\n",
    "    data = json.dumps(parameters)\n",
    "    request = requests.post(url, headers=headers, data=data, auth=('apikey', ))        \n",
    "    json_object = request.json()\n",
    "    \n",
    "    \n",
    "    if 'entities' in json_object:  \n",
    "        print(json_object['entities'])\n",
    "        for entity in json_object['entities']:\n",
    "            if \"mentions\" in entity:\n",
    "                for mention in entity['mentions']:\n",
    "                    processedList = [video]\n",
    "                    processedList.append(entity['text'].encode('utf-8').decode('utf-8'))\n",
    "                    processedList.append(entity['type'])\n",
    "                    processedList.append(entity['count'])\n",
    "                    processedList.append(mention['location'][0])\n",
    "                    processedList.append(mention['location'][1])\n",
    "                    processedList.append(entity['relevance'])\n",
    "                    if 'disambiguation' in entity:\n",
    "                        if 'subtype' in entity['disambiguation']:\n",
    "                            processedList.append(\", \".join(entity['disambiguation']['subtype']))\n",
    "                        else:\n",
    "                            processedList.append(\"NONE\")\n",
    "                        if 'dbpedia_resource' in entity['disambiguation']:\n",
    "                            processedList.append(entity['disambiguation']['dbpedia_resource'])\n",
    "                        else:\n",
    "                            processedList.append(\"NONE\")\n",
    "                    else:\n",
    "                        processedList.append(\"NONE\")\n",
    "                        processedList.append(\"NONE\")\n",
    "                    processedList.append(entity['sentiment']['label'])\n",
    "                    processedList.append(entity['sentiment']['score'])\n",
    "\n",
    "                    if 'emotion' in entity:\n",
    "                        processedList.append(entity['emotion']['sadness'])\n",
    "                        processedList.append(entity['emotion']['fear'])\n",
    "                        processedList.append(entity['emotion']['joy'])\n",
    "                        processedList.append(entity['emotion']['disgust'])\n",
    "                        processedList.append(entity['emotion']['anger'])\n",
    "                    else:\n",
    "                        processedList.append(0.0)\n",
    "                        processedList.append(0.0)\n",
    "                        processedList.append(0.0)\n",
    "                        processedList.append(0.0)\n",
    "                        processedList.append(0.0)\n",
    "\n",
    "                    with open(\"..data/video_subtitles_entities/\" + video + \".csv\",, \"a\") as outSentences:\n",
    "                        writer = csv.writer( outSentences )\n",
    "                        writer.writerow(processedList)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
